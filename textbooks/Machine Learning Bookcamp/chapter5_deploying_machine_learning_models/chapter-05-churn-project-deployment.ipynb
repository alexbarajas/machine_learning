{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a7501b3e-67e5-46cd-b43f-81079bda7b3a",
   "metadata": {},
   "source": [
    "# Deploying the churn prediction project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a91f089a-773c-4e48-92c3-a88a359bd14a",
   "metadata": {},
   "source": [
    "# 5. Deploying machine learning models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f3baeed-17e7-4511-aa87-8c07c98d6ee0",
   "metadata": {},
   "source": [
    "Model deployment is the process of putting models to use. This chapter is about packaging a model inside a web service so that other services can use it. We will also see how to deploy the web service to a production-ready environment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbbbca57-933f-490c-8c5e-8b6e1a29ee73",
   "metadata": {},
   "source": [
    "## 5.1. Churn-prediction model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "92c5943e-c61b-4d64-b11f-4e983bb02f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "263048e9-1e0c-440f-9139-6a286c656d24",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../chapter3_machine_learning_for_classification/Telco-Customer-Churn.csv\")\n",
    "\n",
    "df[\"TotalCharges\"] = pd.to_numeric(df[\"TotalCharges\"], errors=\"coerce\")\n",
    "df[\"TotalCharges\"] = df[\"TotalCharges\"].fillna(0)\n",
    "\n",
    "df.columns = df.columns.str.lower().str.replace(\" \", \"_\")\n",
    "string_columns = list(df.dtypes[df.dtypes == \"object\"].index)\n",
    "for column in string_columns:\n",
    "    df[column] = df[column].str.lower().str.replace(\" \", \"_\")\n",
    "\n",
    "df.churn = (df.churn == \"yes\").astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fd173f52-037f-44c3-aa2f-d9ef2cd09c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_full, df_test = train_test_split(df, test_size=0.2, random_state=1)\n",
    "\n",
    "df_train_full = df_train_full.reset_index(drop=True)\n",
    "df_test = df_test.reset_index(drop=True)\n",
    "\n",
    "df_train, df_val = train_test_split(df_train_full, test_size=0.33, random_state=11)\n",
    "\n",
    "df_train = df_train.reset_index(drop=True)\n",
    "df_val = df_val.reset_index(drop=True)\n",
    "\n",
    "y_train = df_train.churn.values\n",
    "y_val = df_val.churn.values\n",
    "\n",
    "del df_train[\"churn\"]\n",
    "del df_val[\"churn\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "af4e315c-577c-4327-b54e-4e3b3499c4f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical = ['gender', 'seniorcitizen', 'partner', 'dependents',\n",
    "               'phoneservice', 'multiplelines', 'internetservice',\n",
    "               'onlinesecurity', 'onlinebackup', 'deviceprotection',\n",
    "               'techsupport', 'streamingtv', 'streamingmovies',\n",
    "               'contract', 'paperlessbilling', 'paymentmethod']\n",
    "numerical = ['tenure', 'monthlycharges', 'totalcharges']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "752e1a5a-8372-4348-a7e8-4802624245bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(df, y, C=1.0):\n",
    "    cat = df[categorical + numerical].to_dict(orient=\"records\")\n",
    "    \n",
    "    dv = DictVectorizer(sparse=False)\n",
    "    dv.fit(cat)\n",
    "\n",
    "    X = dv.transform(cat)\n",
    "\n",
    "    model = LogisticRegression(solver=\"liblinear\", C=C)\n",
    "    model.fit(X, y)\n",
    "\n",
    "    return dv, model\n",
    "\n",
    "def predict(df, dv, model):\n",
    "    # applies the same one-hot encoding scheme as in the train function\n",
    "    cat = df[categorical + numerical].to_dict(orient=\"records\")\n",
    "    \n",
    "    X = dv.transform(cat)\n",
    "\n",
    "    # uses the model to make predictions\n",
    "    y_pred = model.predict_proba(X)[:, 1]\n",
    "\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3d87ad55-2349-48c7-8bc3-915467267db0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "auc = 0.858\n"
     ]
    }
   ],
   "source": [
    "y_train = df_train_full[\"churn\"].values\n",
    "y_test = df_test[\"churn\"].values\n",
    "\n",
    "dv, model = train(df_train_full, y_train, C=0.5)\n",
    "y_pred = predict(df_test, dv, model)\n",
    "\n",
    "auc = roc_auc_score(y_test, y_pred)\n",
    "print(\"auc = %.3f\" % auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4982a4dd-188c-4f47-85f8-b120e3dcc870",
   "metadata": {},
   "source": [
    "### 5.1.1. Using the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "93bfc79e-4b87-43ea-992b-1d78a60cfe07",
   "metadata": {},
   "outputs": [],
   "source": [
    "customer = {\n",
    "    'customerid': '8879-zkjof',\n",
    "    'gender': 'female',\n",
    "    'seniorcitizen': 0,\n",
    "    'partner': 'no',\n",
    "    'dependents': 'no',\n",
    "    'tenure': 41,\n",
    "    'phoneservice': 'yes',\n",
    "    'multiplelines': 'no',\n",
    "    'internetservice': 'dsl',\n",
    "    'onlinesecurity': 'yes',\n",
    "    'onlinebackup': 'no',\n",
    "    'deviceprotection': 'yes',\n",
    "    'techsupport': 'yes',\n",
    "    'streamingtv': 'yes',\n",
    "    'streamingmovies': 'yes',\n",
    "    'contract': 'one_year',\n",
    "    'paperlessbilling': 'yes',\n",
    "    'paymentmethod': 'bank_transfer_(automatic)',\n",
    "    'monthlycharges': 79.85,\n",
    "    'totalcharges': 3320.75\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60bb27b5-365f-4b8d-86a9-773179e67bc8",
   "metadata": {},
   "source": [
    "To predict whether this customer is going to churn, we can use the predict function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d4953243-eef7-43d6-9a4c-b796740a26b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.05960531889903496)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame([customer])\n",
    "y_pred = predict(df, dv, model)\n",
    "y_pred[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92915fef-431d-47d6-9690-ea05513b7676",
   "metadata": {},
   "source": [
    "The current predict function is inefficient if we're only using one customer with it because we turn that customer dictionary into a dataframe, only to convert that dataframe back to a dictionary later. We can create a separate function for predicting the probability of churn for a single customer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7ba60900-8b67-4490-aceb-0c192351f6c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_single(customer, dv, model):\n",
    "    X = dv.transform([customer])\n",
    "    y_pred = model.predict_proba(X)[:, 1]\n",
    "    return y_pred[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d517ee0c-f7bb-4b69-b842-26214bf72b69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.05960531889903496)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_single(customer, dv, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bd2ce4e-d9ee-4728-b04d-fa810ce449da",
   "metadata": {},
   "source": [
    "The result is the same, this customer has a 6% probability of churning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b25d6b5-d78e-4f6e-a603-35cb377e4b18",
   "metadata": {},
   "source": [
    "### 5.1.2. Using Pickle to save and load the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c14d760f-e161-476c-99fb-8283ac1742c8",
   "metadata": {},
   "source": [
    "To be able to use our models outside of a notebook, we need to save it, and then later, another process can load and use it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52ec4821-3a17-4f36-b18a-927188ef6bea",
   "metadata": {},
   "source": [
    "Pickle is a serialization/deserialization module that's already built into Python. Using it, we can save an arbitrary Python object (with a few exceptions) to a file. Once we have a file, we can load the model from there in a different process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "593c58a5-90c0-45bd-ac84-a3540ccd7790",
   "metadata": {},
   "source": [
    "Pickling an object in Python means saving it using the Pickle module."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "847ec1a6-eb84-47a2-ae87-6a31486e9453",
   "metadata": {},
   "source": [
    "To save a model, we first import the Pickle module, and then use the dump function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "40a3a741-6272-4e1f-81d2-a2a945cb0e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"churn-model.bin\", \"wb\") as f_out:\n",
    "    pickle.dump(model, f_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4119077e-7392-41c2-b06e-6c0e94e32381",
   "metadata": {},
   "source": [
    "To save the model, we use the open function, which takes two arguments:\n",
    "\n",
    "- The name of the file that we want to open. For us, it’s churn-model.bin.\n",
    "- The mode with which we open the file. For us, it’s wb, which means we want to write to the file (w), and the file is binary (b) and not text—Pickle uses binary format for writing to files.\n",
    "\n",
    "The open function returns f_out—the file descriptor we can use to write to the file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6eb2434-73ec-44ee-b617-b01a9ac3da46",
   "metadata": {},
   "source": [
    "Next, we use the dump function from Pickle. It also takes two arguments:\n",
    "\n",
    "- The object we want to save. For us, it’s model.\n",
    "- The file descriptor, pointing to the output file, which is f_out for us."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b607f252-fd7b-4c22-92cf-daf74eb76ee1",
   "metadata": {},
   "source": [
    "Finally, we use the with construction in this code. When we open a file with open, we need to close it after we finish writing. When using with, it happens automatically."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eca143a-877a-40e7-a51a-b336c8534a65",
   "metadata": {},
   "source": [
    "In our case, howevver, saving just the model is not enough: we also have a DictVectorizer that we \"trained\" together with the model. We need to save both."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "185e431e-8e45-415f-a2be-73c6656b08fe",
   "metadata": {},
   "source": [
    "The simplest way of doing this is to put borh of them in a tuple when pcikling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bfee5870-ecfa-4b82-8b0e-9b614edbd246",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"churn-model.bin\", \"wb\") as f_out:\n",
    "    pickle.dump((dv, model), f_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3869887-0d1a-4793-bf9f-88c62524d76b",
   "metadata": {},
   "source": [
    "To load the model, we use the laod function from Pickle. We can test it in the same Jupyter Notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d09cdd72-e082-409d-9a04-8505a9fc3987",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"churn-model.bin\", \"rb\") as f_in:\n",
    "    dv, model = pickle.load(f_in)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "899f02bb-7a4c-4a5c-bc72-900483069159",
   "metadata": {},
   "source": [
    "We again use the open function, but this time, with a different mode: rb, which means we open it for reading (r), and the file is binary (b)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "539510a7-2d1e-402f-b4eb-407ff3290e15",
   "metadata": {},
   "source": [
    "Be careful when specifying the mode. Accidentally specifying an incorrect mode may result in data loss: if you open an existing file with the w mode instead of r, it will overwrite the content."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e52a63d1-ef47-4aad-bd33-cdde498ec14c",
   "metadata": {},
   "source": [
    "Because we saved a tuple, we unpack it when loading, so we get both the vectorizer and the model at the same time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a07d3d50-06e5-4c1e-aadc-66e3789b3f1e",
   "metadata": {},
   "source": [
    "Let's create a simple Python script that loads the model and applies it to a customer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0bd56f5-0e29-4d14-8990-33c06569e1d1",
   "metadata": {},
   "source": [
    "We will call this file churn_serving.py. It contains:\n",
    "\n",
    "- The predict_simple function that we wrote earlier.\n",
    "- The code for loading the model.\n",
    "- The code for applying the model to a customer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a0b7927-f41a-4be5-8693-18e5a3b69afc",
   "metadata": {},
   "source": [
    "We can run the file like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2b7f70fc-fff5-434d-9435-65a267ce7ff7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction: 0.060\n",
      "verdict: Not churn\n"
     ]
    }
   ],
   "source": [
    "%run churn_serving.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3100e682-f17a-4968-b217-49513f481071",
   "metadata": {},
   "source": [
    "This way, we can load the model and apply it to the customer we specified in the script."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e878e63d-deef-430a-906e-74a33f2f07bc",
   "metadata": {},
   "source": [
    "## 5.2. Model serving"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdba13df-6f56-40a8-8e7f-e3cf8529ac6c",
   "metadata": {},
   "source": [
    "We already know how to load a trained model in a different process. Now we need to serve this model-make it available for others to use."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e7affcc-5428-467e-b795-9415fbb3a7d9",
   "metadata": {},
   "source": [
    "Here, we will do it in Python with Flask-a Python framework for creating web services."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf75210f-1663-4b48-b6a7-4f447152a640",
   "metadata": {},
   "source": [
    "### 5.2.1. Web services"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "400dd112-8503-4e02-969d-dfb37b500f95",
   "metadata": {},
   "source": [
    "We already know how to use a model to make a prediction, but so far, we have simply hardcoded the features of a customer as a Python dictionary. Let’s try to imagine how our model will be used in practice."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "351175f6-f26e-4dfd-b36a-82101b5f93f3",
   "metadata": {},
   "source": [
    "Suppose we have a service for running marketing campaigns. For each customer, it needs to determine the probability of churn, and if it's high enough, it will send a promotional email with discounts. Of course, this service needs to use our model to decide whether it should send an email."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17fd2ff3-6317-46ff-8d72-4e6e8010ca94",
   "metadata": {},
   "source": [
    "One possible way of achieving this is to modify the code of the campaign service: load the model, and score the customers right in the service. This approach is good, but the campaign service needs to be in Python, and we need to have full control over its code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2c2d64d-e45d-4b38-b94b-40e2d61c9afd",
   "metadata": {},
   "source": [
    "Unfortunately, this situation is not always the case: it may be written in some other language, or a different team might be in charge of this project, which means we won’t have the control we need."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0f4d5cd-dd7a-4fe4-9764-a662e490b4e4",
   "metadata": {},
   "source": [
    "The typical solution for this problem is putting a model inside a web service—a small service (a microservice) that only takes care of scoring customers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b0f63e1-702c-421e-80f2-f26fa393353e",
   "metadata": {},
   "source": [
    "So, we need to create a churn service-a service in Python that will serve the churn model. Given the features of a customer, it will respond with the probability of churn for this customer. For each customer, the campaign service will ask the churn service for the probability of churn, and if it's high enough, then we send a promotional email."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ce41a74-363e-4d29-a088-0962ecfefada",
   "metadata": {},
   "source": [
    "This gives us another advantage: separation of concerns. If the model is created by data scientists, then they can take ownership of the service and maintain it, while the other team takes care of the campaign service."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce67da89-a0d4-4aea-8cd4-515510d43ad6",
   "metadata": {},
   "source": [
    "One of the most popular frameworks for creating web services in Python is Flask."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd3ba316-7e68-4418-a720-0de085b076ce",
   "metadata": {},
   "source": [
    "### 5.2.2. Flask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3542e556-d4df-4633-ba60-39f9b276dc1c",
   "metadata": {},
   "source": [
    "The easiest way to implement a web service in Python is to use Flask. It's quite lightweight, requires little code to get started, and hides most of the complexity of dealing with HTTP requests and responses."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2446015f-2981-403a-961c-24f40f655a1f",
   "metadata": {},
   "source": [
    "Let's make a Flask file and call it flask_test.py."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63180d37-00f7-4bc1-b604-5f2209c96b7d",
   "metadata": {},
   "source": [
    "### 5.2.3. Serving churn model with Flask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "73b9e18e-056c-4c74-8dee-0d0650dd2c89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'churn': False, 'churn_probability': 0.05960531889903496}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "url = 'http://localhost:9696/predict'\n",
    "response = requests.post(url, json=customer)\n",
    "result = response.json()\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac1d6bc0-0ab4-49cb-8f73-c126a2b0f4a5",
   "metadata": {},
   "source": [
    "If the campaign service used Python, this is exactly how it could communicate with the churn service and decide who should get promotional emails."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2790b2b-f94f-41b6-8d55-a0922e267024",
   "metadata": {},
   "source": [
    "## 5.3. Managing dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c79a732f-f296-4eaa-8960-a75407b85d5a",
   "metadata": {},
   "source": [
    "### 5.3.1. Pipenv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93f77607-398d-4da6-936c-8cbd9add6cf1",
   "metadata": {},
   "source": [
    "To serve the churn model, we only need a few libraries: NumPy, Scikit-learn, and Flask."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f05e88d4-c8ac-4b92-8652-a013aa5af9a7",
   "metadata": {},
   "source": [
    "Pipenv is a tool that makes managing virtual environments easier. We can install it with pip:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "bbd90f04-c8b8-43c4-b63d-d41eb42b00ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install pipenv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeaddf5a-31be-453f-bace-87acb8c255b9",
   "metadata": {},
   "source": [
    "After that, we use pipenv instead of pip for installing dependencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5c6e25ad-9a1a-4340-8383-02927dd57daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipenv install numpy scikit-learn flask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51a545c5-55db-47bc-a77f-bcb15c642379",
   "metadata": {},
   "source": [
    "After all the libraries are installed, we need to activate the virtual environment-this way, our applicatoni will use the correct versions of the libraries. We do it by running the shell command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "70d70f39-99d3-4a8b-bf02-adc089322f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipenv shell"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccae58d5-8818-4970-8c06-943f36d9997d",
   "metadata": {},
   "source": [
    "Instead of first explicitly entering the virtual environment and then running the script, we can perform these two steps with just one command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "bb9d8c6c-e74b-4c40-8155-8719495ae24a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipenv run python churn_serving.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8296d22-83af-428f-99eb-653128fb96bd",
   "metadata": {},
   "source": [
    "The run command in Pipenv simply runs the specified program in the virtual environment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59660845-d4ca-4e66-b8d1-49ea32b54544",
   "metadata": {},
   "source": [
    "### 5.3.2. Docker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e14eb40c-784c-408e-af11-c651e504aec6",
   "metadata": {},
   "source": [
    "Docker solves the \"but it works on my machine\" problem by aso packaging the OS and the system libraries into a Docker container–a self-contained environment that works anywhere where Docker is installed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0418bbd9-7a4d-4e62-a82d-b1d0a6006192",
   "metadata": {},
   "source": [
    "Once the service is packaged into a Docker container, we can run it on the host machine—our laptop (regardless of the OS) or any public cloud provider."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3fb10a3-2ab7-4c32-bc90-240f527a9503",
   "metadata": {},
   "source": [
    "I won't do much Docker stuff because I want to focus on machine learning, but this chapter has some good stuff if you want to read it in the future."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae645d45-705a-41d9-97b7-e9a8cfbde46d",
   "metadata": {},
   "source": [
    "Docker makes it easy to run services in a reproducible way. With Docker, the environment inside the container always stays the same. This means that if we can run our service on a laptop, it will work anywhere else."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20ad2a13-2ce4-485a-ade4-0f7cbe68de39",
   "metadata": {},
   "source": [
    "## 5.4. Deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f1c1cec-d073-456e-8c37-c55c9a1eeaa1",
   "metadata": {},
   "source": [
    "We don’t run production services on our laptops; we need special servers for that."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e6182d2-be5c-41e8-9910-ac3a3eafa8ac",
   "metadata": {},
   "source": [
    "Some services you can use are AWS, Google Cloud, Microsoft Azure, and Digital Ocean. I won't do this section for now, so let's move on."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
